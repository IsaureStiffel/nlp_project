{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 : Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\", index_col = 'index')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df , df['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = ['fantasy', 'science', 'crime', 'history', 'horror', 'thriller', 'psychology', 'romance', 'sports', 'travel']\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# use the sns.kdeplot function to visualize text length for each class # https://seaborn.pydata.org/generated/seaborn.kdeplot.html\n",
    "for label in class_labels:\n",
    "    sns.kdeplot(df[df['genre'] == label]['summary'].str.len(), fill=True, label=label)\n",
    "plt.legend()\n",
    "plt.title(\"Is a genre notably longer or shorter ? \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Travel and sport have slightly shorter summary than the rest of the genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed'] = df['summary'].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_column = \" \".join(df['processed'])\n",
    "split_column = join_column.split(' ')\n",
    "\n",
    "cnt = Counter(split_column)\n",
    "most_common = cnt.most_common()\n",
    "most_common[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a column without the text words to analyse impact on future results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQWORDS = [w for (w, word_count) in most_common[:20]]\n",
    "\n",
    "def remove_freqwords(text: str, freq_words: list=FREQWORDS) -> str:\n",
    "    cleaned_text = \" \".join([word for word in text.split() if word not in freq_words])\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_wo_freq\"] = df[\"processed\"].apply(remove_freqwords)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out words with a frequency of 1\n",
    "filtered_words = [word for word, frequency in most_common if frequency == 1]\n",
    "\n",
    "print(len(filtered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing all the words appearing only one time because they are not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAREWORDS = [word for word, frequency in most_common if frequency == 1]\n",
    "\n",
    "def remove_rarewords(text: str, rare_words: list=RAREWORDS) -> str:\n",
    "    cleaned_text = \" \".join([word for word in text.split() if word not in rare_words])\n",
    "    return cleaned_text\n",
    "\n",
    "df[\"text_wo_freq_rare\"] = df[\"text_wo_freq\"].apply(remove_rarewords)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text: str) -> str:\n",
    "\n",
    "    # return a string where each word has been stemmed\n",
    "    split = text.split()\n",
    "\n",
    "    # Apply the stemmer to the words\n",
    "    stem_words = [stemmer.stem(word) for word in split]\n",
    "\n",
    "    # Join the stemmes words into a stemmed text\n",
    "    stem_text = ' '.join(stem_words)\n",
    "\n",
    "    return stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_stemmed\"] = df[\"text_wo_freq_rare\"].apply(stem_words)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text: str) -> str:\n",
    "\n",
    "    # Initialize a mapping of POS tags to WordNet tags\n",
    "    wordnet_map = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }\n",
    "\n",
    "    # Use the nltk.pos_tag fucntion to get the POS tags of every word in the input\n",
    "    # https://www.nltk.org/api/nltk.tag.pos_tag.html\n",
    "    # You may also use nltk.word_tokenize to tokenize the text instead of split()\n",
    "    # https://www.nltk.org/api/nltk.tokenize.html\n",
    "    pos_tagged_text = nltk.pos_tag(nltk.tokenize.word_tokenize(text))\n",
    "\n",
    "    # Return the lemmatized version of the text\n",
    "    # Inside the lemmatize function, use the (word, POS tag) tuple collected in the pos_tagged_text list\n",
    "    # hint: query the wordnet_map (wordnet_map.get(... , ...)) using the pos tag, return wordnet.NOUN as a default\n",
    "    lemmatized_words = [lemmatizer.lemmatize(words, pos = wordnet_map.get(pos[0], wordnet.NOUN)) for words, pos in pos_tagged_text ]\n",
    "    \n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    # Return the lemmatized version of the text\n",
    "    return lemmatized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_lemmatized\"] = df[\"text_wo_freq_rare\"].apply(lemmatize_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_w_lemmatizing = ' '.join(df[\"text_lemmatized\"]).split()\n",
    "all_text_w_stemming = ' '.join(df[\"text_stemmed\"]).split()\n",
    "\n",
    "n_words_no_stemming = len(set(all_text_w_lemmatizing))\n",
    "n_words_w_stemming = len(set(all_text_w_stemming))\n",
    "vocabulary_size_diff = n_words_no_stemming - n_words_w_stemming\n",
    "\n",
    "print(f\"Number of unique words with lemmatizing: {n_words_no_stemming}\")\n",
    "print(f\"Number of unique words with stemming: {n_words_w_stemming}\")\n",
    "print(f\"Difference: {vocabulary_size_diff} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will keep stemmed words rather than lemmatized because it has less unique words. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
